{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # Data processing, CSV file I/O \n",
    "import numpy as np # Linear Algebra\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "# The various layers for the Neural Network Model.\n",
    "# Dense - A layer that is fully connected (densely-connected.)\n",
    "# Conv2D - A 2-dimensional convolutional layer.\n",
    "# Dropout - A layer that helps prevent overfitting.\n",
    "# Flatten - A layer that flattens the input.\n",
    "# MaxPooling2D - A layer that performs Max Pooling of the Convolutions\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "# Since I have a GPU & I've GPU enabled, I am going to use the GPU version of keras \n",
    "# (NOTE: Ignore if you do not have GPU enabled)\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "#K.tensorflow_backend._get_available_gpus()\n",
    "\n",
    "modelLoaded = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = pd.read_csv(filepath_or_buffer=\"data.csv\", \n",
    "#                      sep=\",\")\n",
    "\n",
    "#trainSet, testSet = train_test_split(dataset,test_size = 0.2)\n",
    "\n",
    "#trainSet.to_pickle(\"train_set.pkl\")\n",
    "#testSet.to_pickle(\"test_set.pkl\")\n",
    "\n",
    "#print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = pd.read_pickle(\"train_set.pkl\")\n",
    "testSet = pd.read_pickle(\"test_set.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"Models\\\\CNN_01.model\")\n",
    "modelLoaded = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Divisione ed elaborazione del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "trainSet_x = trainSet.drop(\"Label\",axis = 1).div(255.0)\n",
    "trainSet_label = trainSet[\"Label\"]\n",
    "\n",
    "testSet_x = testSet.drop(\"Label\",axis = 1).div(255.0)\n",
    "testSet_label = testSet[\"Label\"]\n",
    "\n",
    "Labels = trainSet_label.unique()\n",
    "i=0\n",
    "intToLabel = {}\n",
    "labelToInt = {}\n",
    "for l in Labels:\n",
    "    intToLabel[i] = str(l)\n",
    "    labelToInt[l] = i\n",
    "    i+=1\n",
    "    \n",
    "trainSet_x = trainSet_x.values.reshape(trainSet.shape[0],28,28,1)\n",
    "testSet_x = testSet_x.values.reshape(testSet.shape[0],28,28,1)\n",
    "\n",
    "trainSet_y = []\n",
    "for l in trainSet_label:\n",
    "    trainSet_y.append(labelToInt[l])\n",
    "\n",
    "testSet_y = []\n",
    "for l in testSet_label:\n",
    "    testSet_y.append(labelToInt[l])\n",
    "    \n",
    "print(len(Labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Creazione e training del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLoaded = False\n",
    "INPUT_SHAPE = trainSet_x[0].shape\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 2\n",
    "TOT_LABELS = max(trainSet_y)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               6423040   \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 84)                43092     \n",
      "=================================================================\n",
      "Total params: 6,531,124\n",
      "Trainable params: 6,531,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "if not modelLoaded:\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same', input_shape=INPUT_SHAPE))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same', input_shape=INPUT_SHAPE))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "    \n",
    "    #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(TOT_LABELS, activation='softmax'))\n",
    "    \n",
    "    print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 270701 samples, validate on 30078 samples\n",
      "Epoch 1/2\n",
      "270701/270701 [==============================] - 1678s 6ms/step - loss: 0.3019 - acc: 0.9164 - val_loss: 0.1357 - val_acc: 0.9574\n",
      "Epoch 2/2\n",
      "270701/270701 [==============================] - 1964s 7ms/step - loss: 0.0983 - acc: 0.9688 - val_loss: 0.0807 - val_acc: 0.9744\n"
     ]
    }
   ],
   "source": [
    "if not modelLoaded:\n",
    "    model.compile(optimizer=\"adam\",\n",
    "                 loss=\"sparse_categorical_crossentropy\",\n",
    "                 metrics=[\"accuracy\"])\n",
    "    model.fit(x=trainSet_x,\n",
    "              y=trainSet_y,\n",
    "              epochs=EPOCHS,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not modelLoaded:\n",
    "    model.save(\"CNN.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testSet_x)\n",
    "print(testSet_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75195/75195 [==============================] - 174s 2ms/step\n",
      "['loss', 'acc']\n",
      "[0.08272785814927966, 0.973562071946273]\n"
     ]
    }
   ],
   "source": [
    "value = model.evaluate(testSet_x, testSet_y)\n",
    "\n",
    "print(model.metrics_names)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(testSet_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.53028791807967\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "i=0\n",
    "for y in testSet_label:\n",
    "    p = np.argmax(predictions[i])\n",
    "    #print(str(y) + \" - \" + intToLabel[p])\n",
    "    if intToLabel[p] == str(y):\n",
    "        correct += 1\n",
    "    i+=1\n",
    " \n",
    "print((correct/len(predictions))*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
